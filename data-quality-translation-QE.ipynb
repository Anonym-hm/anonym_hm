{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "u6DIxyxRbIRe",
   "metadata": {
    "id": "u6DIxyxRbIRe"
   },
   "source": [
    "# Data Quality Evaluation\n",
    "\n",
    "In this notebook, we evaluate the quality of translations with quality estimation (QE) metric called CometKiwi22.\n",
    "That model was trained on data realeased within WMT 2022 Shared Task and supports many languages, including French.\n",
    "\n",
    "The model is developped by Unbabel; the model card can be found here: https://huggingface.co/Unbabel/wmt22-cometkiwi-da."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f467e91",
   "metadata": {},
   "source": [
    "We run all the code on a single GPU A100. Running this notebook took ~10 mins. \n",
    "The corresponding results are reported in Chapter 4 of the paper in Table 2: Translation quality per sentence category es-\n",
    "timated with COMETKIWI22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n3OjHvo-CHcB",
   "metadata": {
    "id": "n3OjHvo-CHcB"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dPto9Q2fA8Ug",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPto9Q2fA8Ug",
    "outputId": "2cfd6df0-4cd5-44c2-f055-0c13d62d3c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification\tgeneration  moral_stories_full.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls \"data_fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spNWUmBbb9KF",
   "metadata": {
    "id": "spNWUmBbb9KF"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Crq-NR-WBM_p",
   "metadata": {
    "id": "Crq-NR-WBM_p"
   },
   "outputs": [],
   "source": [
    "french_data_dir=\"data_fr\"\n",
    "english_data_dir=\"data_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iXn8AFeGBLFj",
   "metadata": {
    "id": "iXn8AFeGBLFj"
   },
   "outputs": [],
   "source": [
    "records_fr = []\n",
    "with open(french_data_dir+\"/moral_stories_full.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        records_fr.append(json.loads(line))\n",
    "french_df = pd.DataFrame(records_fr)\n",
    "records_en = []\n",
    "with open(english_data_dir+\"/moral_stories_full.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        records_en.append(json.loads(line))\n",
    "english_df = pd.DataFrame(records_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eHUMr3KI9GW3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHUMr3KI9GW3",
    "outputId": "6b0ffdc2-c1f8-4eb4-ce27-49e6387eb019"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X0lF2eY1Bwzz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0lF2eY1Bwzz",
    "outputId": "dc6a3f50-76fe-4fa7-c8e3-d4feed72e2a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'norm', 'situation', 'intention', 'moral_action',\n",
       "       'moral_consequence', 'immoral_action', 'immoral_consequence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XOgnSSAHDQQD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOgnSSAHDQQD",
    "outputId": "96e8370d-d3ea-47eb-b5f6-67492f0179ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12000it [00:23, 516.05it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for index, row_en in tqdm(english_df.iterrows()):\n",
    "    en_id = row_en['ID']\n",
    "    row_fr = french_df[french_df['ID'] == en_id].iloc[0]  # Assuming there's exactly one match\n",
    "    for column in english_df.columns:\n",
    "        data_entry = {}\n",
    "        if column != 'ID':\n",
    "            data_entry[\"src\"] = row_en[column]\n",
    "            data_entry[\"mt\"] = row_fr[column]\n",
    "        if data_entry:\n",
    "            data.append(data_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7xGiWSsLb72D",
   "metadata": {
    "id": "7xGiWSsLb72D"
   },
   "source": [
    "# Load model for QE\n",
    "\n",
    "Note, that before loading the model you have to loging on HF and accept with license to be granted with acceess: https://huggingface.co/Unbabel/wmt22-cometkiwi-da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NIs94YtucZUd",
   "metadata": {
    "id": "NIs94YtucZUd"
   },
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN=\"KEY\" #TB copied from https://huggingface.co/settings/tokens. READ access token is enough to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tKAjLuMCDzs-",
   "metadata": {
    "id": "tKAjLuMCDzs-"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip  # ensures that pip is current\n",
    "!pip install \"unbabel-comet>=2.0.0\" -q\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YJgHnp8hFGj4",
   "metadata": {
    "id": "YJgHnp8hFGj4"
   },
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bXHlPVdcE_2k",
   "metadata": {
    "id": "bXHlPVdcE_2k"
   },
   "outputs": [],
   "source": [
    "model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ugboCDM7-eyL",
   "metadata": {
    "id": "ugboCDM7-eyL"
   },
   "outputs": [],
   "source": [
    "data=[{ \"src\": \"It is wrong to worry your grandmother\",\n",
    "        \"mt\": \"Il est mal de s'inquiéter pour sa grand-mère.\"\n",
    "    }] # Example of how the model can be called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mGafs_HS9b1z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGafs_HS9b1z",
    "outputId": "b5982abb-a436-49e6-e003-5b4c19d73a1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction([('scores', [0.6662602424621582]),\n",
       "            ('system_score', 0.6662602424621582)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data, batch_size=8, gpus=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XSPUqZWZdm9d",
   "metadata": {
    "id": "XSPUqZWZdm9d"
   },
   "source": [
    "For the input pairs of sentences, model outputs a score as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lLmOCHg1Jfq1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLmOCHg1Jfq1",
    "outputId": "b40f8207-2405-4f78-931d-96624c65c481"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12000it [00:20, 572.29it/s]\n"
     ]
    }
   ],
   "source": [
    "data_all=dict()\n",
    "for index, row_en in tqdm(english_df.iterrows()):\n",
    "    en_id = row_en['ID']\n",
    "    row_fr = french_df[french_df['ID'] == en_id].iloc[0]\n",
    "    for column in english_df.columns:\n",
    "        if column != 'ID':\n",
    "            data_entry = {}\n",
    "            if column not in data_all.keys():\n",
    "                data_all[column]=[]\n",
    "            data_entry[\"src\"] = row_en[column]\n",
    "            data_entry[\"mt\"] = row_fr[column]\n",
    "            if data_entry:\n",
    "                data_all[column].append(data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qDlTd0gZLOgR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDlTd0gZLOgR",
    "outputId": "3d316b2b-fe05-455d-8160-8e387c6e9596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm\n",
      "situation\n",
      "intention\n",
      "moral_action\n",
      "moral_consequence\n",
      "immoral_action\n",
      "immoral_consequence\n"
     ]
    }
   ],
   "source": [
    "for k,v in data_all.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ht3pH-hYdTfI",
   "metadata": {
    "id": "ht3pH-hYdTfI"
   },
   "source": [
    "# Run evaluation on paired translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YeoQBmHbJfhx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeoQBmHbJfhx",
    "outputId": "78a743e8-6bff-4a64-80c0-89a0864455d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1500/1500 [01:36<00:00, 15.51it/s]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "situation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1500/1500 [02:25<00:00, 10.30it/s]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1500/1500 [01:25<00:00, 17.49it/s]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moral_action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1500/1500 [02:19<00:00, 10.78it/s]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moral_consequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1500/1500 [02:12<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immoral_action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 1500/1500 [02:23<00:00, 10.46it/s]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immoral_consequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1500/1500 [02:19<00:00, 10.73it/s]\n"
     ]
    }
   ],
   "source": [
    "scores_all=dict()\n",
    "for k,v in data_all.items():\n",
    "    print(k)\n",
    "    model_output_i = model.predict(v, batch_size=8, gpus=1)\n",
    "    scores_all[k]=model_output_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gjjwYWXsOdNl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjjwYWXsOdNl",
    "outputId": "7d49967d-7572-4654-cfca-219a446bc83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm & 0.858\n",
      "situation & 0.85\n",
      "intention & 0.854\n",
      "moral_action & 0.844\n",
      "moral_consequence & 0.848\n",
      "immoral_action & 0.832\n",
      "immoral_consequence & 0.841\n"
     ]
    }
   ],
   "source": [
    "for k,v in scores_all.items():\n",
    "    value_=np.mean(v['scores'])\n",
    "    string_=str(k)+\" & \" + str(round(value_, 3))\n",
    "    print(string_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Ecll2G7I-IE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ecll2G7I-IE",
    "outputId": "fceecea4-3d4c-4d34-9e59-f3361269c74b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm & 0.057\n",
      "situation & 0.043\n",
      "intention & 0.049\n",
      "moral_action & 0.046\n",
      "moral_consequence & 0.045\n",
      "immoral_action & 0.054\n",
      "immoral_consequence & 0.052\n"
     ]
    }
   ],
   "source": [
    "for k,v in scores_all.items():\n",
    "    value_=np.std(v['scores'])\n",
    "    string_=str(k)+\" & \" + str(round(value_, 3))\n",
    "    print(string_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
